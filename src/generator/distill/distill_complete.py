import json
import os
from openai import OpenAI
from dotenv import load_dotenv
from prompt_matrix import *
from topic_tree import topic_tree, topic_tree_hash, translate_topic_path
from task_tree import task_tree
import json_repair
from tqdm import tqdm
import random
from concurrent.futures import ThreadPoolExecutor, as_completed

# åˆå§‹åŒ– OpenAI clientï¼ˆéœ€åœ¨å‡½æ•°å¤–éƒ¨ï¼‰
load_dotenv()
client = OpenAI(
    api_key=os.getenv("QWEN_API_KEY"),
    base_url=os.getenv("QWEN_URL"),
)

def format_multi_docs(sample):
    """
    ç”ŸæˆåŒ…å«ä¸»æ–‡æ¡£å’Œä¸å®šæ•°é‡ç›¸å…³æ–‡æ¡£çš„æ ¼å¼åŒ–å­—ç¬¦ä¸²

    å‚æ•°:
        main_doc (str): ä¸»æ–‡æ¡£å†…å®¹
        relevant_docs (list): ç›¸å…³æ–‡æ¡£åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ æ˜¯ä¸€ä¸ªæ–‡æ¡£å­—ç¬¦ä¸²

    è¿”å›:
        str: æŒ‰æŒ‡å®šæ ¼å¼ç»„åˆçš„å®Œæ•´å­—ç¬¦ä¸²
    """
    main_doc = doc_str_format.format(title=sample["metadata"].get("Title", ""), content=sample["contents"])
    relevant_docs = sample["relevant_contents"]
    parts = [f"### ä¸»æ–‡æ¡£\n{main_doc}\n"]

    # å¾ªç¯æ·»åŠ ç›¸å…³æ–‡æ¡£ï¼ˆæ ¹æ®åˆ—è¡¨é•¿åº¦åŠ¨æ€ç”Ÿæˆï¼‰
    for i, doc in enumerate(relevant_docs, start=0):
        doc = doc_str_format.format(title=doc["metadata"].get("Title", ""), content=doc["contents"])
        parts.append(f"### ç›¸å…³æ–‡æ¡£ {i}\n{doc}\n")

    # æ‹¼æ¥æ‰€æœ‰éƒ¨åˆ†ï¼Œå¹¶ç”¨strip()å»é™¤é¦–å°¾å¤šä½™ç©ºè¡Œ
    return ''.join(parts).strip()

def load_jsonl(file_path: str, start: int = 0, end: int | None = None):
    """è¯»å–JSONLæ–‡ä»¶å¹¶è¿”å›æ–‡æ¡£åˆ—è¡¨"""
    docs = []
    if not os.path.exists(file_path):
        raise FileNotFoundError(f"è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")

    with open(file_path, "r", encoding="utf-8") as f:
        for line_idx, line in enumerate(tqdm(f, desc="è¯»å–è¾“å…¥JSONL"), 0):
            if line_idx < start:
                continue
            if end is not None and line_idx >= end:
                break
            line_num = line_idx + 1
            line = line.strip()
            if not line:
                continue
            try:
                doc = json.loads(line)
                if not all(key in doc for key in ["id", "contents", "metadata", "relevant_contents"]):
                    print(f"âš ï¸ è·³è¿‡ç¬¬{line_num}è¡Œ: ç¼ºå°‘å¿…éœ€å­—æ®µ(id/contents/metadata/relevant_contents)")
                    continue
                docs.append(doc)
            except json.JSONDecodeError as e:
                print(f"âš ï¸ è·³è¿‡ç¬¬{line_num}è¡Œ: JSONè§£æé”™è¯¯ - {str(e)[:50]}")

    print(f"âœ… æˆåŠŸè¯»å– {len(docs)} ä¸ªæœ‰æ•ˆæ–‡æ¡£")
    return docs

def pipeline_demo(text_sample, candidate_workers: int = 1):
    """
    ç®€åŒ–ç‰ˆ pipeline æµç¨‹
    1. topic åˆ†ç±»
    2. task åˆ†ç±»
    3. æ•°æ®ç”Ÿæˆ
    4. æ•°æ®è¿‡æ»¤
    """
    # filter the funding data
    if isinstance(text_sample["contents"], dict) and random.random() < 0.8:
        print("è‚¡ç¥¨æ•°æ®ï¼Œå·²è¿‡æ»¤")
        return

    # res
    res = [{}]
    res[0]["id"] = text_sample["id"]
    res[0]["contents"] = text_sample["contents"]
    res[0]["metadata"] = text_sample["metadata"]

    # ------------------ 1. Topic ------------------
    # print("Topic Configuration", topic_tree)
    topic_user_input = topic_classify_user.format(
        title=text_sample["metadata"].get("Title", " "),
        content=text_sample["contents"],
        topics_str=topic_tree
    )
    topic_messages = [
        {"role": "system", "content": topic_classify_system},
        {"role": "user", "content": topic_user_input}
    ]

    completion = client.chat.completions.create(
        model="qwen3-30b-a3b-instruct-2507",
        messages=topic_messages,
        temperature=0.1,
        extra_body={"enable_thinking": False},
        stream=False
    )
    topic_response = json.loads(completion.choices[0].message.content.strip())
    topic_id = int(topic_response["topic_id"])
    if topic_id == 0:
        print("Topic ID=0ï¼Œæµç¨‹ç»ˆæ­¢")
        return None
    res[0]["topic"] = topic_tree_hash[topic_id]
    print("Approporate topic for this context:\n",topic_tree_hash[topic_id])
    # print("Current res after the topic:\n", res)

    # ------------------ 2. Task åˆ†ç±» ------------------
    task_strs = []
    task_hash = []
    for i, (task_name, desc) in enumerate(task_tree.items()):
        try:
            desc = desc.split("### ä»»åŠ¡è¦æ±‚")[1].strip()
        except IndexError:
            desc = "æœªæ‰¾åˆ°æ˜ç¡®ä»»åŠ¡è¦æ±‚"
        task_strs.append(json.dumps({"id": i, "name": task_name, "description": desc}, ensure_ascii=False))
        task_hash.append(task_name)
    # print("task configuration", task_strs, task_hash)

    # Combine the content and the relevant refence contents 
    doc_str = format_multi_docs(text_sample)
    # print("The context for distill process",doc_str)

    task_user_input = task_classify_user.format(
        doc_str=doc_str,
        task_str="\n".join(task_strs),
        topic_str=res[0]["topic"]
    )
    task_messages = [
        {"role": "system", "content": task_classify_system},
        {"role": "user", "content": task_user_input}
    ]

    completion = client.chat.completions.create(
        model="qwen-plus",#"qwen3-next-80b-a3b-thinking", #"qwen3-30b-a3b",
        messages=task_messages,
        extra_body={"enable_thinking": False},
        temperature=0.2,
        stream=False
    )
    task_response = json.loads(completion.choices[0].message.content.strip())
    # print(task_response)
    task_ids = task_response["task_id_list"]

    if not task_ids:  # æ²¡æœ‰ç»“æœ
        print("No suitable task for this content, break up.")
        return []

    new_res = []
    for r in res:
        for i, task_id in enumerate(task_ids):
            if task_id < len(task_hash):
                new_r = r.copy()  # æ‹·è´å·²æœ‰ç»“æœ
                new_r["task"] = task_hash[task_id]

                new_r["relevant_contents_idxs"] = task_response["selected_relevant_contents_idx"][i]
                new_r["relevant_contents_ids"] = [text_sample["relevant_contents"][idx_content]["id"] for idx_content in task_response["selected_relevant_contents_idx"][i]]
                new_res.append(new_r)
            else:
                print(f"âš ï¸ task_id {task_id} è¶…å‡ºèŒƒå›´ï¼Œå¿½ç•¥")
    res = new_res
    print("Approporate topic for this task:\n",task_ids)
    # print("Current res after the task:\n", res)

    # ------------------ 3. æ•°æ®ç”Ÿæˆ ------------------
    def generate_for_candidate(candidate):
        task_name = candidate["task"]

        main_doc = doc_str_format.format(
            title=candidate["metadata"].get("Title", ""),
            content=candidate["contents"]
        )
        parts = [f"### æ–‡æ¡£\n{main_doc}\n"]

        relevant_node_ids = [candidate["id"]]

        for i, relevant_doc_idx in enumerate(candidate["relevant_contents_idxs"]):
            relevant_doc = text_sample["relevant_contents"][relevant_doc_idx]
            doc = doc_str_format.format(
                title=relevant_doc["metadata"].get("Title", ""),
                content=relevant_doc["contents"]
            )
            parts.append(f"### ç›¸å…³æ–‡æ¡£ {i}\n{doc}\n")
            relevant_node_ids.append(relevant_doc["id"])

        doc_str = ''.join(parts).strip()
        print(f"\n{'='*50}\nä»»åŠ¡ç±»å‹: {task_name}\n{'='*50}")
        print(f"æ–‡æ¡£æ•°é‡: 1 + {len(candidate['relevant_contents_idxs'])} ä¸ªç›¸å…³æ–‡æ¡£")

        task_require = task_tree[task_name].replace("### ä»»åŠ¡è¦æ±‚", "").strip()

        generation_user_input = data_generation_user.format(
            topic_name=candidate["topic"],
            task_name=task_name,
            task_require=task_require,
            doc_str=doc_str
        )

        generation_messages = [
            {"role": "system", "content": data_generation_system},
            {"role": "user", "content": generation_user_input}
        ]

        try:
            completion = client.chat.completions.create(
                model="qwen-plus",
                messages=generation_messages,
                extra_body={"enable_thinking": False},
                temperature=0.2,
                stream=False
            )

            raw_response = completion.choices[0].message.content.strip()
            final_response = json_repair.loads(raw_response)

            if not isinstance(final_response, list):
                final_response = [final_response]

            print(f"ç”Ÿæˆæ•°æ®æ•°é‡: {len(final_response)}")

            generated_data = process_task_output(
                task_name=task_name,
                candidate=candidate,
                final_response=final_response,
                relevant_node_ids=relevant_node_ids
            )

            if generated_data:
                print(f"âœ… æˆåŠŸç”Ÿæˆ {len(generated_data)} æ¡ {task_name} æ•°æ®")
                return generated_data
            print(f"âš ï¸ ä»»åŠ¡ {task_name} æœªç”Ÿæˆæœ‰æ•ˆæ•°æ®")
            return []

        except Exception as e:
            print(f"âŒ è§£ææˆ–å¤„ç†æ•°æ®æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}ï¼Œè·³è¿‡æœ¬è½®å¤„ç†")
            import traceback
            traceback.print_exc()
            return []

    res_generations = []
    if candidate_workers and candidate_workers > 1 and len(res) > 1:
        with ThreadPoolExecutor(max_workers=candidate_workers) as executor:
            future_to_candidate = {executor.submit(generate_for_candidate, c): c for c in res}
            for future in as_completed(future_to_candidate):
                generated_data = future.result()
                if generated_data:
                    res_generations.extend(generated_data)
    else:
        for candidate in res:
            generated_data = generate_for_candidate(candidate)
            if generated_data:
                res_generations.extend(generated_data)

    return res_generations

def process_task_output(task_name, candidate, final_response, relevant_node_ids):
    """
    æ ¹æ®ä¸åŒä»»åŠ¡ç±»å‹å¤„ç†è¾“å‡ºæ ¼å¼
    
    ä»»åŠ¡ç±»å‹ä¸è¾“å‡ºæ ¼å¼å¯¹åº”å…³ç³»ï¼š
    - æŠ½å–ç±»é—®ç­”: å•æ¡é—®ç­”ï¼Œanswerä¸ºåˆ—è¡¨
    - å¤šè·³æ¨ç†ç±»é—®ç­”: å•æ¡é—®ç­”ï¼Œéœ€è¦å¤šæ­¥æ¨ç†
    - å¯¹æ¯”ç±»é—®ç­”: å•æ¡é—®ç­”ï¼Œæ¶‰åŠå¯¹æ¯”
    - é•¿ç­”æ¡ˆå½¢å¼é—®ç­”: å•æ¡é—®ç­”ï¼Œç­”æ¡ˆè¾ƒé•¿
    - å¤šè½®å¯¹è¯èƒ½åŠ›: å¤šè½®å¯¹è¯åˆ—è¡¨æ ¼å¼
    """
    results = []
    
    if not final_response or len(final_response) == 0:
        return results
    
    topic_name = candidate["topic"]
    
    if task_name == "å¤šè½®å¯¹è¯èƒ½åŠ›":
        # å¤šè½®å¯¹è¯ï¼šæ•´ä¸ªresponseä½œä¸ºä¸€ä¸ªå¯¹è¯åºåˆ—
        # è¾“å‡ºæ ¼å¼: [{"question": ..., "answer": ..., "relevant_passage": ...}, ...]
        conversation_turns = []
        for turn in final_response:
            if not validate_qa_item(turn):
                continue
            turn_data = {
                "question": turn["question"],
                "answer": ensure_list(turn["answer"]),
                "relevant_passage": ensure_list(turn.get("relevant_passage", [])),
                "topic_name": topic_name,
                "task_name": task_name,
                "relevant_node": relevant_node_ids
            }
            conversation_turns.append(turn_data)
        
        if len(conversation_turns) >= 2:  # è‡³å°‘2è½®æ‰ç®—å¤šè½®å¯¹è¯
            results.append(conversation_turns)
    else:
        # å…¶ä»–ä»»åŠ¡ç±»å‹ï¼šæ¯æ¡responseç‹¬ç«‹å¤„ç†
        for item in final_response:
            if not validate_qa_item(item):
                continue
            
            data_item = {
                "question": item["question"],
                "answer": ensure_list(item["answer"]),
                "relevant_passage": ensure_list(item.get("relevant_passage", [])),
                "topic_name": topic_name,
                "task_name": task_name,
                "relevant_node": relevant_node_ids if len(relevant_node_ids) > 1 else relevant_node_ids[0]
            }
            results.append(data_item)
    
    return results

def validate_qa_item(item):
    """éªŒè¯é—®ç­”é¡¹æ˜¯å¦æœ‰æ•ˆ"""
    if not isinstance(item, dict):
        return False
    if "question" not in item or "answer" not in item:
        return False
    if not item["question"] or not item["answer"]:
        return False
    # æ’é™¤æ— æ•ˆç­”æ¡ˆ
    invalid_answers = ["æ— ", "ç©º", "æ— æ³•å›ç­”", "æ— æ³•æ ¹æ®æ£€ç´¢æ–‡æ¡£å›ç­”é—®é¢˜"]
    answer = item["answer"]
    if isinstance(answer, str) and answer in invalid_answers:
        return False
    if isinstance(answer, list) and len(answer) == 1 and answer[0] in invalid_answers:
        return False
    return True

def ensure_list(value):
    """ç¡®ä¿å€¼ä¸ºåˆ—è¡¨æ ¼å¼"""
    if value is None:
        return []
    if isinstance(value, list):
        return value
    return [value]

def get_task_filename(task_name):
    """æ ¹æ®ä»»åŠ¡åç§°è·å–å¯¹åº”çš„æ–‡ä»¶å"""
    task_file_mapping = {
        "å¤šè·³æ¨ç†ç±»é—®ç­”": "multi-hop-reasoning.jsonl",
        "å¯¹æ¯”ç±»é—®ç­”": "contrastive.jsonl",
        "é•¿ç­”æ¡ˆå½¢å¼é—®ç­”": "long-form.jsonl",
        "å¤šè½®å¯¹è¯èƒ½åŠ›": "conversational.jsonl"
    }
    return task_file_mapping.get(task_name, "other.jsonl")

def save_generation_results(results, output_dir, topic_name=None):
    """
    ä¿å­˜ç”Ÿæˆç»“æœåˆ°å¯¹åº”çš„æ–‡ä»¶
    
    Args:
        results: ç”Ÿæˆçš„æ•°æ®åˆ—è¡¨
        output_dir: è¾“å‡ºç›®å½•
        topic_name: å¯é€‰çš„ä¸»é¢˜åç§°ï¼Œç”¨äºåˆ›å»ºå­ç›®å½•
    """
    if not results:
        print("æ²¡æœ‰æ•°æ®éœ€è¦ä¿å­˜")
        return
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    if topic_name:
        # å°†ä¸­æ–‡ä¸»é¢˜åç§°è½¬æ¢ä¸ºè‹±æ–‡ç›®å½•è·¯å¾„
        english_topic_path = translate_topic_path(topic_name)
        output_path = os.path.join(output_dir, english_topic_path)
    else:
        output_path = output_dir
    
    os.makedirs(output_path, exist_ok=True)
    
    # æŒ‰ä»»åŠ¡ç±»å‹åˆ†ç»„ä¿å­˜
    task_groups = {}
    for item in results:
        if isinstance(item, list):  # å¤šè½®å¯¹è¯
            task_name = item[0]["task_name"] if item else "unknown"
        else:
            task_name = item.get("task_name", "unknown")
        
        if task_name not in task_groups:
            task_groups[task_name] = []
        task_groups[task_name].append(item)
    
    # ä¿å­˜åˆ°å¯¹åº”æ–‡ä»¶
    for task_name, items in task_groups.items():
        filename = get_task_filename(task_name)
        filepath = os.path.join(output_path, filename)
        
        with open(filepath, "a", encoding="utf-8") as f:
            for item in items:
                f.write(json.dumps(item, ensure_ascii=False) + "\n")
        
        print(f"ğŸ“ å·²ä¿å­˜ {len(items)} æ¡ {task_name} æ•°æ®åˆ° {filepath}")

def run_batch_pipeline(input_jsonl_path, output_dir, max_docs=None, start: int = 0, end: int | None = None, candidate_workers: int = 1):
    """
    æ‰¹é‡å¤„ç†JSONLæ–‡ä»¶ä¸­çš„æ–‡æ¡£
    
    Args:
        input_jsonl_path: è¾“å…¥JSONLæ–‡ä»¶è·¯å¾„
        output_dir: è¾“å‡ºç›®å½•
        max_docs: æœ€å¤§å¤„ç†æ–‡æ¡£æ•°ï¼ˆå¯é€‰ï¼‰
    """
    files = load_jsonl(input_jsonl_path, start=start, end=end)
    
    if max_docs:
        files = files[:max_docs]
    
    total_generated = 0
    
    for i, file in enumerate(tqdm(files, desc="è’¸é¦å¤„ç†è¿›åº¦", unit="doc")):
        doc_id = file.get("id", f"doc_{i}")
        try:
            results = pipeline_demo(file, candidate_workers=candidate_workers)
            
            if results and len(results) > 0:
                # è·å–ä¸»é¢˜åç§°ç”¨äºåˆ†ç±»ä¿å­˜
                if isinstance(results[0], list):
                    topic_name = results[0][0].get("topic_name") if results[0] else None
                else:
                    topic_name = results[0].get("topic_name")
                
                save_generation_results(results, output_dir, topic_name)
                total_generated += len(results)
                print(f"âœ… æ–‡æ¡£ {doc_id}: ç”Ÿæˆ {len(results)} æ¡æ•°æ®")
            else:
                print(f"ğŸ“„ æ–‡æ¡£ {doc_id}: æ— æœ‰æ•ˆç”Ÿæˆæ•°æ®")
                
        except Exception as e:
            print(f"âŒ æ–‡æ¡£ {doc_id}: å¤„ç†å¤±è´¥ - {str(e)[:100]}")
            import traceback
            traceback.print_exc()
            continue
    
    print(f"\n{'='*50}")
    print(f"å¤„ç†å®Œæˆ! å…±å¤„ç† {len(files)} ä¸ªæ–‡æ¡£ï¼Œç”Ÿæˆ {total_generated} æ¡æ•°æ®")
    print(f"{'='*50}")

# ------------------ ä¸»ç¨‹åºå…¥å£ ------------------
if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="é‡‘èé¢†åŸŸRAGè¯„æµ‹æ•°æ®è’¸é¦ç”Ÿæˆå·¥å…·")
    parser.add_argument("--mode", type=str, default="demo", choices=["demo", "batch"],
                        help="è¿è¡Œæ¨¡å¼: demo(å•æ–‡æ¡£æµ‹è¯•) æˆ– batch(æ‰¹é‡å¤„ç†)")
    parser.add_argument("--input", type=str, default=None,
                        help="è¾“å…¥JSONLæ–‡ä»¶è·¯å¾„ (batchæ¨¡å¼å¿…éœ€)")
    parser.add_argument("--output", type=str, default="./output/rag_generation",
                        help="è¾“å‡ºç›®å½•è·¯å¾„")
    parser.add_argument("--max_docs", type=int, default=None,
                        help="æœ€å¤§å¤„ç†æ–‡æ¡£æ•° (å¯é€‰)")
    parser.add_argument("--candidate_workers", type=int, default=1,
                        help="task candidateå¹¶å‘æ•°(>1å¯ç”¨å¹¶è¡Œ)")
    parser.add_argument("--start", type=int, default=0,
                        help="è’¸é¦èµ·å§‹è¡Œå·(0-based, inclusive)ï¼Œbatchæ¨¡å¼æœ‰æ•ˆ")
    parser.add_argument("--end", type=int, default=None,
                        help="è’¸é¦æˆªæ­¢è¡Œå·(0-based, exclusive)ï¼Œbatchæ¨¡å¼æœ‰æ•ˆ")
    
    args = parser.parse_args()
    
    if args.mode == "demo":
        # å•æ–‡æ¡£æµ‹è¯•æ¨¡å¼
        print("=" * 60)
        print("è¿è¡Œæ¨¡å¼: å•æ–‡æ¡£æµ‹è¯• (demo)")
        print("=" * 60)
        
        text_sample = {
            "id": "b3ee734d-2417-42d1-b0b8-45ae35e92a28",
            "contents": "å›½å®¶å¤–æ±‡ç®¡ç†å±€å‰¯å±€é•¿ã€æ–°é—»å‘è¨€äººç‹æ˜¥è‹±å°±2022å¹´ä¸ŠåŠå¹´å›½é™…æ”¶æ”¯çŠ¶å†µç­”è®°è€…é—®_ç®¡ç†èµ„è®¯_é’å²›å¸‚åˆ†å±€ æ—¥å‰ï¼Œå›½å®¶å¤–æ±‡ç®¡ç†å±€å…¬å¸ƒäº†2022å¹´äºŒå­£åº¦åŠä¸ŠåŠå¹´å›½é™…æ”¶æ”¯å¹³è¡¡è¡¨åˆæ­¥æ•°æ®ã€‚å›½å®¶å¤–æ±‡ç®¡ç†å±€å‰¯å±€é•¿ã€æ–°é—»å‘è¨€äººç‹æ˜¥è‹±å°±ç›¸å…³é—®é¢˜å›ç­”äº†è®°è€…æé—®ã€‚ é—®ï¼š2022å¹´ä¸ŠåŠå¹´æˆ‘å›½å›½é™…æ”¶æ”¯çŠ¶å†µæœ‰ä½•ç‰¹ç‚¹ï¼Ÿ ç­”ï¼šå›½é™…æ”¶æ”¯å¹³è¡¡è¡¨åˆæ­¥æ•°æ®æ˜¾ç¤ºï¼Œ2022å¹´ä¸ŠåŠå¹´æˆ‘å›½å›½é™…æ”¶æ”¯ä¿æŒåŸºæœ¬å¹³è¡¡ã€‚å…¶ä¸­ï¼Œç»å¸¸è´¦æˆ·é¡ºå·®1691äº¿ç¾å…ƒï¼Œä¸åŒæœŸå›½å†…ç”Ÿäº§æ€»å€¼ï¼ˆgdpï¼‰ä¹‹æ¯”ä¸º1.9%ï¼Œç»§ç»­å¤„äºåˆç†å‡è¡¡åŒºé—´ï¼›ç›´æ¥æŠ•èµ„å‡€æµå…¥749äº¿ç¾å…ƒï¼Œä¿æŒåœ¨è¾ƒé«˜æ°´å¹³ã€‚",
            "metadata": {"source_file": "190879.pkl", "Title": "2022å¹´ä¸ŠåŠå¹´å›½é™…æ”¶æ”¯çŠ¶å†µ"},
            "relevant_contents": [
                {
                    "id": "c538bfb2", 
                    "contents": "å›½å®¶å¤–æ±‡ç®¡ç†å±€å‰¯å±€é•¿ã€æ–°é—»å‘è¨€äººç‹æ˜¥è‹±å°±2022å¹´ä¸ŠåŠå¹´å›½é™…æ”¶æ”¯çŠ¶å†µç­”è®°è€…é—®_ç®¡ç†èµ„è®¯_å¹¿è¥¿å£®æ—è‡ªæ²»åŒºåˆ†å±€\næ—¥å‰ï¼Œå›½å®¶å¤–æ±‡ç®¡ç†å±€å…¬å¸ƒäº†2022å¹´äºŒå­£åº¦åŠä¸ŠåŠå¹´å›½é™…æ”¶æ”¯å¹³è¡¡è¡¨åˆæ­¥æ•°æ®ã€‚å›½å®¶å¤–æ±‡ç®¡ç†å±€å‰¯å±€é•¿ã€æ–°é—»å‘è¨€äººç‹æ˜¥è‹±å°±ç›¸å…³é—®é¢˜å›ç­”äº†è®°è€…æé—®ã€‚\né—®ï¼š2022å¹´ä¸ŠåŠå¹´æˆ‘å›½å›½é™…æ”¶æ”¯çŠ¶å†µæœ‰ä½•ç‰¹ç‚¹ï¼Ÿ\nç­”ï¼šå›½é™…æ”¶æ”¯å¹³è¡¡è¡¨åˆæ­¥æ•°æ®æ˜¾ç¤ºï¼Œ2022å¹´ä¸ŠåŠå¹´æˆ‘å›½å›½é™…æ”¶æ”¯ä¿æŒåŸºæœ¬å¹³è¡¡ã€‚",
                    "metadata": {"source_file": "363002.pkl", "Title": "å¹¿è¥¿åˆ†å±€å›½é™…æ”¶æ”¯é—®ç­”"}
                }
            ]
        }
        
        results = pipeline_demo(text_sample, candidate_workers=args.candidate_workers)
        
        if results:
            print(f"\n{'='*60}")
            print(f"ç”Ÿæˆç»“æœé¢„è§ˆ (å…± {len(results)} æ¡):")
            print("=" * 60)
            for i, item in enumerate(results[:3]):  # åªæ˜¾ç¤ºå‰3æ¡
                print(f"\n--- ç¬¬ {i+1} æ¡ ---")
                if isinstance(item, list):  # å¤šè½®å¯¹è¯
                    print(f"ç±»å‹: å¤šè½®å¯¹è¯ ({len(item)} è½®)")
                    for j, turn in enumerate(item[:2]):  # æ˜¾ç¤ºå‰2è½®
                        print(f"  è½®æ¬¡{j+1} Q: {turn['question'][:50]}...")
                else:
                    print(f"ç±»å‹: {item.get('task_name', 'unknown')}")
                    print(f"é—®é¢˜: {item['question'][:80]}...")
                    answer = item['answer'][0] if item['answer'] else ""
                    print(f"ç­”æ¡ˆ: {answer[:80]}...")
            
            # ä¿å­˜ç»“æœ
            os.makedirs(args.output, exist_ok=True)
            save_generation_results(results, args.output)
        else:
            print("æœªç”Ÿæˆæœ‰æ•ˆæ•°æ®")
            
    elif args.mode == "batch":
        # æ‰¹é‡å¤„ç†æ¨¡å¼
        if not args.input:
            print("é”™è¯¯: batchæ¨¡å¼éœ€è¦æŒ‡å®š --input å‚æ•°")
            exit(1)
        
        print("=" * 60)
        print(f"è¿è¡Œæ¨¡å¼: æ‰¹é‡å¤„ç† (batch)")
        print(f"è¾“å…¥æ–‡ä»¶: {args.input}")
        print(f"è¾“å‡ºç›®å½•: {args.output}")
        if args.max_docs:
            print(f"æœ€å¤§æ–‡æ¡£æ•°: {args.max_docs}")
        if args.candidate_workers and args.candidate_workers > 1:
            print(f"candidateå¹¶å‘æ•°: {args.candidate_workers}")
        if args.start or args.end is not None:
            print(f"å¤„ç†è¡ŒèŒƒå›´: [{args.start}, {args.end})")
        print("=" * 60)
        
        run_batch_pipeline(args.input, args.output, args.max_docs, start=args.start, end=args.end, candidate_workers=args.candidate_workers)
