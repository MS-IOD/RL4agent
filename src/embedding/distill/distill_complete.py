# import json
# import os
# import time
# import traceback
# from typing import List, Dict
# from tqdm import tqdm
# from openai import OpenAI, OpenAIError
# from dotenv import load_dotenv

# from prompt import emd_stage1, emd_stage2
# from build_persona_db import PersonaRetriever

# def load_jsonl(file_path: str) -> List[Dict]:
#     """è¯»å–JSONLæ–‡ä»¶å¹¶è¿”å›æ–‡æ¡£åˆ—è¡¨"""
#     docs = []
#     if not os.path.exists(file_path):
#         raise FileNotFoundError(f"è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")

#     with open(file_path, "r", encoding="utf-8") as f:
#         for line_num, line in enumerate(tqdm(f, desc="è¯»å–è¾“å…¥JSONL"), 1):
#             line = line.strip()
#             if not line:
#                 continue
#             try:
#                 doc = json.loads(line)
#                 if not all(key in doc for key in ["id", "contents"]):
#                     print(f"âš ï¸ è·³è¿‡ç¬¬{line_num}è¡Œ: ç¼ºå°‘id/contentså­—æ®µ")
#                     continue
#                 docs.append(doc)
#             except json.JSONDecodeError as e:
#                 print(f"âš ï¸ è·³è¿‡ç¬¬{line_num}è¡Œ: JSONè§£æé”™è¯¯ - {str(e)[:50]}")

#     print(f"âœ… æˆåŠŸè¯»å– {len(docs)} ä¸ªæœ‰æ•ˆæ–‡æ¡£")
#     # return docs[50010:50020]
#     return docs


# def append_single_doc_to_jsonl(doc: Dict, file_path: str) -> None:
#     """å•ä¸ªæ–‡æ¡£è¿½åŠ å†™å…¥JSONLæ–‡ä»¶ï¼ˆä¸æ“¦é™¤åŸæœ‰æ•°æ®ï¼‰"""
#     with open(file_path, "a", encoding="utf-8") as f:
#         json.dump(doc, f, ensure_ascii=False)
#         f.write("\n")


# def init_llm_client() -> OpenAI:
#     """åˆå§‹åŒ–LLMå®¢æˆ·ç«¯"""
#     load_dotenv()
#     api_key = os.getenv("QWEN_API_KEY")
#     base_url = os.getenv("QWEN_URL")

#     if not api_key or not base_url:
#         raise ValueError("âŒ è¯·åœ¨.envæ–‡ä»¶ä¸­é…ç½®QWEN_API_KEYå’ŒQWEN_URL")

#     try:
#         return OpenAI(api_key=api_key, base_url=base_url)
#     except Exception as e:
#         raise RuntimeError(f"âŒ LLMå®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥ï¼š{str(e)}") from e


# def process_single_doc(
#         doc: Dict,
#         llm_client: OpenAI,
#         persona_retriever: PersonaRetriever
# ) -> Dict:
#     """å¤„ç†å•ä¸ªæ–‡æ¡£ï¼Œä¿®å¤è§’è‰²æ£€ç´¢å’Œæ ¼å¼é”™è¯¯"""
#     try:
#         # 1. å®‰å…¨è·å–æ–‡æ¡£å†…å®¹ï¼ˆé˜²æ­¢contentsä¸ºç©ºæˆ–éå­—ç¬¦ä¸²ï¼‰
#         doc_contents = str(doc.get("contents", "")).strip()
#         if not doc_contents:
#             raise ValueError("æ–‡æ¡£å†…å®¹ä¸ºç©ºï¼Œæ— æ³•å¤„ç†")

#         # 2. é¢„å¤„ç†æ–‡æ¡£
#         passage = {
#             "id": doc["id"],
#             "contents": doc_contents
#         }
#         passage_str = json.dumps(passage, ensure_ascii=False)

#         # 3. æ£€ç´¢å€™é€‰è§’è‰²ï¼ˆæ ¸å¿ƒä¿®å¤ï¼šéªŒè¯è¿”å›ç»“æœæ ¼å¼ï¼‰
#         try:
#             # é™åˆ¶query_texté•¿åº¦ï¼Œé¿å…è¶…é•¿æ–‡æœ¬å¯¼è‡´æ£€ç´¢é”™è¯¯
#             query_text = doc_contents[:256] if len(doc_contents) > 256 else doc_contents
#             candidates = persona_retriever.retrieve_similar_personas(
#                 query_text=query_text,  # ç¡®ä¿ä¼ å…¥çš„æ˜¯å­—ç¬¦ä¸²è€Œéåˆ‡ç‰‡
#                 top_k=5
#             )

#             # éªŒè¯candidatesæ ¼å¼ï¼šå¿…é¡»æ˜¯åˆ—è¡¨ï¼Œä¸”å…ƒç´ ä¸ºå«"persona"é”®çš„å­—å…¸
#             if not isinstance(candidates, list):
#                 raise TypeError(f"è§’è‰²æ£€ç´¢è¿”å›éåˆ—è¡¨ç±»å‹ï¼š{type(candidates)}")

#             valid_personas = []
#             for idx, item in enumerate(candidates):
#                 if not isinstance(item, dict) or "persona" not in item:
#                     print(f"âš ï¸ è¿‡æ»¤æ— æ•ˆè§’è‰²æ•°æ®ï¼ˆç¬¬{idx + 1}ä¸ªï¼‰ï¼š{str(item)[:50]}")
#                     continue
#                 # ç¡®ä¿personaæ˜¯å­—ç¬¦ä¸²
#                 persona_str = str(item["persona"]).strip()
#                 if persona_str:
#                     valid_personas.append(persona_str)

#             if not valid_personas:
#                 raise ValueError("æœªè·å–åˆ°æœ‰æ•ˆè§’è‰²æ•°æ®ï¼Œæ— æ³•ç»§ç»­å¤„ç†")

#             characters = "ï¼›".join(valid_personas)
#             print(f"ğŸ“Œ æœ‰æ•ˆè§’è‰²æ•°ï¼š{len(valid_personas)}")

#         except Exception as e:
#             raise RuntimeError(f"è§’è‰²æ£€ç´¢å¤±è´¥ï¼š{str(e)}") from e

#         # 4. Stage1ï¼šç”Ÿæˆè§’è‰²ã€é—®é¢˜ç±»å‹ã€éš¾åº¦
#         stage1_prompt = emd_stage1.format(
#             passage=passage_str,
#             characters=characters
#         )
#         stage1_resp = llm_client.chat.completions.create(
#             model=LLM_MODEL,
#             messages=[
#                 {"role": "system", "content": SYSTEM_PROMPT},
#                 {"role": "user", "content": stage1_prompt}
#             ],
#             extra_body={"enable_thinking": False},
#             temperature=0.1,
#             stream=False
#         )

#         # éªŒè¯Stage1è¿”å›æ˜¯å¦ä¸ºJSON
#         stage1_content = stage1_resp.choices[0].message.content.strip()
#         try:
#             stage1_result = json.loads(stage1_content)
#         except json.JSONDecodeError as e:
#             raise ValueError(f"Stage1è¿”å›éJSONæ ¼å¼ï¼š{stage1_content[:100]}") from e

#         # æå–Stage1ç»“æœï¼ˆéªŒè¯å¿…è¦å­—æ®µï¼‰
#         required_fields = ["Characters", "Question_Type", "Difficulty"]
#         for field in required_fields:
#             if field not in stage1_result:
#                 raise KeyError(f"Stage1ç»“æœç¼ºå°‘å¿…è¦å­—æ®µï¼š{field}")

#         character = str(stage1_result["Characters"]).strip()
#         question_type = str(stage1_result["Question_Type"]).strip()
#         difficulty = str(stage1_result["Difficulty"]).strip()

#         # 5. Stage2ï¼šç”ŸæˆQuery
#         stage2_prompt = emd_stage2.format(
#             passage=doc_contents,
#             character=character,
#             type=question_type,
#             difficulty=difficulty
#         )
#         stage2_resp = llm_client.chat.completions.create(
#             model=LLM_MODEL,
#             messages=[
#                 {"role": "system", "content": SYSTEM_PROMPT},
#                 {"role": "user", "content": stage2_prompt}
#             ],
#             extra_body={"enable_thinking": False},
#             temperature=0.1,
#             stream=False
#         )

#         # å¤„ç†Stage2ç»“æœ
#         generated_query = stage2_resp.choices[0].message.content.strip()
#         try:
#             query_json = json.loads(generated_query)
#             if isinstance(query_json, dict):
#                 final_query = str(query_json.get("Generated_Query", generated_query)).strip()
#             else:
#                 # å¦‚æœè§£æç»“æœæ˜¯listæˆ–strï¼Œç›´æ¥å½“æˆquery
#                 final_query = str(query_json).strip()
#         except json.JSONDecodeError:
#             # å¦‚æœä¸æ˜¯åˆæ³•JSONï¼Œå°±ç›´æ¥åŸæ ·ä½¿ç”¨
#             final_query = generated_query.strip()

#         # æ„å»ºæœ€ç»ˆç»“æœ
#         final_doc = {
#             "id": doc["id"],
#             "contents": doc_contents,
#             **({"metadata": doc["metadata"]} if "metadata" in doc and isinstance(doc["metadata"], dict) else {}),
#             "character": character,
#             "question_type": question_type,
#             "difficulty": difficulty,
#             "query": final_query
#         }
#         final_doc["_process_status"] = "success"
#         print(f"âœ… æ–‡æ¡£[{doc['id'][:8]}...]å¤„ç†æˆåŠŸ")
#         return final_doc

#     except Exception as e:
#         error_msg = f"{type(e).__name__}: {str(e)[:50]}"
#         print(f"âŒ æ–‡æ¡£[{doc['id'][:8]}...]å¤„ç†å¤±è´¥ï¼š{error_msg}")
#         return {"_process_status": "failed", "id": doc["id"]}


# def main():
#     try:
#         print("=" * 60)
#         print("ğŸš€ å¼€å§‹æ–‡æ¡£ä¸²è¡Œå¤„ç†æµç¨‹ï¼ˆä¿®å¤æ ¼å¼é”™è¯¯ï¼‰")
#         print("=" * 60)

#         # 1. åˆå§‹åŒ–èµ„æº
#         print("\n1. åˆå§‹åŒ–ä¾èµ–èµ„æº")
#         llm_client = init_llm_client()
#         persona_retriever = PersonaRetriever(PERSONA_INDEX_DIR)
#         print("âœ… æ‰€æœ‰ä¾èµ–èµ„æºåˆå§‹åŒ–å®Œæˆ")

#         # 2. è¯»å–è¾“å…¥æ–‡æ¡£
#         print("\n2. è¯»å–è¾“å…¥æ–‡æ¡£")
#         input_docs = load_jsonl(INPUT_JSONL_PATH)
#         if not input_docs:
#             print("âš ï¸ æ— æœ‰æ•ˆæ–‡æ¡£ï¼Œç¨‹åºé€€å‡º")
#             return

#         # 3. ä¸²è¡Œå¤„ç†æ–‡æ¡£
#         print(f"\n3. å¼€å§‹ä¸²è¡Œå¤„ç†ï¼ˆå…±{len(input_docs)}ä¸ªæ–‡æ¡£ï¼‰")
#         success_count = 0
#         failed_count = 0

#         for doc in tqdm(input_docs, desc="ğŸ“Š ä¸²è¡Œå¤„ç†è¿›åº¦"):
#             result = process_single_doc(doc, llm_client, persona_retriever)
#             if result["_process_status"] == "success":
#                 del result["_process_status"]
#                 append_single_doc_to_jsonl(result, OUTPUT_JSONL_PATH)
#                 success_count += 1
#             else:
#                 failed_count += 1
#             time.sleep(0.1)  # é¿å…APIè¯·æ±‚è¿‡äºå¯†é›†

#         # 4. è¾“å‡ºç»Ÿè®¡
#         print("\n4. å¤„ç†ç»Ÿè®¡æŠ¥å‘Š")
#         total_count = len(input_docs)
#         print(f"ğŸ“‹ ç»Ÿè®¡ç»“æœï¼š")
#         print(f"   - æ€»å¤„ç†æ–‡æ¡£æ•°ï¼š{total_count}")
#         print(f"   - æˆåŠŸæ•°ï¼š{success_count}ï¼ˆ{round(success_count / total_count * 100, 1)}%ï¼‰")
#         print(f"   - å¤±è´¥æ•°ï¼š{failed_count}ï¼ˆ{round(failed_count / total_count * 100, 1)}%ï¼‰")
#         print(f"   - è¾“å‡ºæ–‡ä»¶ï¼š{OUTPUT_JSONL_PATH}")

#         print("\n" + "=" * 60)
#         print("ğŸ‰ ä¸²è¡Œå¤„ç†æµç¨‹å®Œæˆ")
#         print("=" * 60)

#     except Exception as e:
#         print(f"\nâŒ ç¨‹åºå…¨å±€å¼‚å¸¸ï¼š{str(e)}")
#         traceback.print_exc()
#         print("=" * 60)


# if __name__ == "__main__":
#     # é…ç½®å‚æ•°
#     INPUT_JSONL_PATH = "./datasets/OmniEval-Corpus/all_data_clean_new2.jsonl"
#     OUTPUT_JSONL_PATH = "./datasets/OmniEval-Corpus/all_data_clean_query.jsonl"
#     PERSONA_INDEX_DIR = "./datasets/persona-hub/finance_persona_index"
#     LLM_MODEL = "qwen3-30b-a3b-instruct-2507"#"qwen3-30b-a3b"
#     # LLM_MODEL = "qwen3-30b-a3b"
#     SYSTEM_PROMPT = "ä½ æ˜¯é‡‘èé¢†åŸŸçš„ä¸“ä¸šåˆ†æåŠ©æ‰‹"

#     main()
#     # file = load_jsonl("../../datasets/OmniEval-Corpus/all_data_clean.jsonl")
#     # print(file)




import json
import os
import time
import traceback
import random
from typing import List, Dict
from concurrent.futures import ThreadPoolExecutor, as_completed
from tqdm import tqdm
from openai import OpenAI, OpenAIError
from dotenv import load_dotenv
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading
from queue import Queue
import copy

from prompt import emd_stage1, emd_stage2
from build_persona_db import PersonaRetriever

def load_jsonl(file_path: str, start: int, end: int) -> List[Dict]:
    """è¯»å–JSONLæ–‡ä»¶å¹¶è¿”å›æ–‡æ¡£åˆ—è¡¨"""
    docs = []
    if not os.path.exists(file_path):
        raise FileNotFoundError(f"è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")

    with open(file_path, "r", encoding="utf-8") as f:
        for line_num, line in enumerate(tqdm(f, desc="è¯»å–è¾“å…¥JSONL"), 1):
            line = line.strip()
            if not line:
                continue
            try:
                doc = json.loads(line)
                if not all(key in doc for key in ["id", "contents"]):
                    print(f"âš ï¸ è·³è¿‡ç¬¬{line_num}è¡Œ: ç¼ºå°‘id/contentså­—æ®µ")
                    continue
                docs.append(doc)
            except json.JSONDecodeError as e:
                print(f"âš ï¸ è·³è¿‡ç¬¬{line_num}è¡Œ: JSONè§£æé”™è¯¯ - {str(e)[:50]}")

    print(f"âœ… æˆåŠŸè¯»å– {len(docs)} ä¸ªæœ‰æ•ˆæ–‡æ¡£")
    return docs[start:end]



class ThreadSafeWriter:
    """çº¿ç¨‹å®‰å…¨çš„æ–‡ä»¶å†™å…¥å™¨"""
    def __init__(self, file_path: str):
        self.file_path = file_path
        self.lock = threading.Lock()
    
    def write_doc(self, doc: Dict) -> None:
        """çº¿ç¨‹å®‰å…¨åœ°å†™å…¥å•ä¸ªæ–‡æ¡£"""
        with self.lock:
            with open(self.file_path, "a", encoding="utf-8") as f:
                json.dump(doc, f, ensure_ascii=False)
                f.write("\n")


def init_llm_client() -> OpenAI:
    """åˆå§‹åŒ–LLMå®¢æˆ·ç«¯"""
    load_dotenv()
    api_key = os.getenv("QWEN_API_KEY")
    base_url = os.getenv("QWEN_URL")

    if not api_key or not base_url:
        raise ValueError("âŒ è¯·åœ¨.envæ–‡ä»¶ä¸­é…ç½®QWEN_API_KEYå’ŒQWEN_URL")

    try:
        return OpenAI(api_key=api_key, base_url=base_url)
    except Exception as e:
        raise RuntimeError(f"âŒ LLMå®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥ï¼š{str(e)}") from e


def process_single_doc_concurrent(
    doc: Dict,
    persona_retriever: PersonaRetriever,
    thread_id: int
) -> Dict:
    """å¹¶å‘å¤„ç†å•ä¸ªæ–‡æ¡£ - æ¯ä¸ªçº¿ç¨‹åˆ›å»ºè‡ªå·±çš„LLMå®¢æˆ·ç«¯"""
    try:
        # æ¯ä¸ªçº¿ç¨‹åˆ›å»ºè‡ªå·±çš„LLMå®¢æˆ·ç«¯ï¼Œé¿å…å¹¶å‘å†²çª
        llm_client = init_llm_client()
        
        # 1. å…ˆæ‹¿åˆ°åŸå§‹ contentsï¼Œå¹¶å¯¹ç»“æ„åŒ–æ•°æ®åšæŠ½æ ·è¿‡æ»¤
        raw_contents = doc.get("contents", "")
        # å¦‚æœæ˜¯å­—å…¸ï¼ˆç»“æ„åŒ–æ•°æ®ï¼‰ï¼Œä»¥ 0.9 æ¦‚ç‡ä¸¢å¼ƒï¼Œåªä¿ç•™å°‘é‡æ ·æœ¬å‚ä¸è’¸é¦
        if isinstance(raw_contents, dict) and random.random() < 0.9:
            raise ValueError("ç»“æ„åŒ–æ–‡æ¡£æŒ‰æŠ½æ ·ç­–ç•¥è¢«è·³è¿‡ï¼Œä¸å‚ä¸è’¸é¦")

        # å†å®‰å…¨è·å–æ–‡æ¡£å†…å®¹ï¼ˆé˜²æ­¢ contents ä¸ºç©ºæˆ–éå­—ç¬¦ä¸²ï¼‰
        doc_contents = str(raw_contents).strip()
        if not doc_contents:
            raise ValueError("æ–‡æ¡£å†…å®¹ä¸ºç©ºï¼Œæ— æ³•å¤„ç†")

        # 2. é¢„å¤„ç†æ–‡æ¡£
        passage = {
            "id": doc["id"],
            "contents": doc_contents
        }
        passage_str = json.dumps(passage, ensure_ascii=False)

        # 3. æ£€ç´¢å€™é€‰è§’è‰²
        try:
            query_text = doc_contents[:256] if len(doc_contents) > 256 else doc_contents
            candidates = persona_retriever.retrieve_similar_personas(
                query_text=query_text,
                top_k=5
            )

            if not isinstance(candidates, list):
                raise TypeError(f"è§’è‰²æ£€ç´¢è¿”å›éåˆ—è¡¨ç±»å‹ï¼š{type(candidates)}")

            valid_personas = []
            for idx, item in enumerate(candidates):
                if not isinstance(item, dict) or "persona" not in item:
                    continue
                persona_str = str(item["persona"]).strip()
                if persona_str:
                    valid_personas.append(persona_str)

            if not valid_personas:
                raise ValueError("æœªè·å–åˆ°æœ‰æ•ˆè§’è‰²æ•°æ®ï¼Œæ— æ³•ç»§ç»­å¤„ç†")

            characters = "ï¼›".join(valid_personas)

        except Exception as e:
            raise RuntimeError(f"è§’è‰²æ£€ç´¢å¤±è´¥ï¼š{str(e)}") from e

        # 4. Stage1ï¼šç”Ÿæˆè§’è‰²ã€é—®é¢˜ç±»å‹ã€éš¾åº¦
        stage1_prompt = emd_stage1.format(
            passage=passage_str,
            characters=characters
        )
        
        max_retries = 3
        for attempt in range(max_retries):
            try:
                stage1_resp = llm_client.chat.completions.create(
                    model=LLM_MODEL,
                    messages=[
                        {"role": "system", "content": SYSTEM_PROMPT},
                        {"role": "user", "content": stage1_prompt}
                    ],
                    extra_body={"enable_thinking": False},
                    temperature=0.1,
                    stream=False,
                    timeout=30  # æ·»åŠ è¶…æ—¶
                )
                break
            except Exception as e:
                if attempt == max_retries - 1:
                    raise e
                time.sleep(1)  # é‡è¯•å‰ç­‰å¾…

        # éªŒè¯Stage1è¿”å›
        stage1_content = stage1_resp.choices[0].message.content.strip()
        try:
            stage1_result = json.loads(stage1_content)
        except json.JSONDecodeError as e:
            raise ValueError(f"Stage1è¿”å›éJSONæ ¼å¼ï¼š{stage1_content[:100]}") from e

        # æå–Stage1ç»“æœ
        required_fields = ["Characters", "Question_Type", "Difficulty"]
        for field in required_fields:
            if field not in stage1_result:
                raise KeyError(f"Stage1ç»“æœç¼ºå°‘å¿…è¦å­—æ®µï¼š{field}")

        character = str(stage1_result["Characters"]).strip()
        question_type = str(stage1_result["Question_Type"]).strip()
        difficulty = str(stage1_result["Difficulty"]).strip()

        # 5. Stage2ï¼šç”ŸæˆQuery
        stage2_prompt = emd_stage2.format(
            passage=doc_contents,
            character=character,
            type=question_type,
            difficulty=difficulty
        )
        
        for attempt in range(max_retries):
            try:
                stage2_resp = llm_client.chat.completions.create(
                    model=LLM_MODEL,
                    messages=[
                        {"role": "system", "content": SYSTEM_PROMPT},
                        {"role": "user", "content": stage2_prompt}
                    ],
                    extra_body={"enable_thinking": False},
                    temperature=0.1,
                    stream=False,
                    timeout=30
                )
                break
            except Exception as e:
                if attempt == max_retries - 1:
                    raise e
                time.sleep(1)

        # å¤„ç†Stage2ç»“æœ
        generated_query = stage2_resp.choices[0].message.content.strip()
        try:
            query_json = json.loads(generated_query)
            if isinstance(query_json, dict):
                final_query = str(query_json.get("Generated_Query", generated_query)).strip()
            else:
                final_query = str(query_json).strip()
        except json.JSONDecodeError:
            final_query = generated_query.strip()

        # æ„å»ºæœ€ç»ˆç»“æœ
        final_doc = {
            "id": doc["id"],
            "contents": doc_contents,
            **({"metadata": doc["metadata"]} if "metadata" in doc and isinstance(doc["metadata"], dict) else {}),
            "character": character,
            "question_type": question_type,
            "difficulty": difficulty,
            "query": final_query
        }
        
        print(f"âœ… [çº¿ç¨‹{thread_id}] æ–‡æ¡£[{doc['id'][:8]}...]å¤„ç†æˆåŠŸ")
        return {"status": "success", "data": final_doc}

    except Exception as e:
        error_msg = f"{type(e).__name__}: {str(e)[:100]}"
        print(f"âŒ [çº¿ç¨‹{thread_id}] æ–‡æ¡£[{doc['id'][:8]}...]å¤„ç†å¤±è´¥ï¼š{error_msg}")
        return {"status": "failed", "id": doc["id"], "error": error_msg}


def process_batch_concurrent(args):
    """å¤„ç†ä¸€æ‰¹æ–‡æ¡£çš„åŒ…è£…å‡½æ•°"""
    docs_batch, persona_retriever, thread_id = args
    results = []
    
    for doc in docs_batch:
        result = process_single_doc_concurrent(doc, persona_retriever, thread_id)
        results.append(result)
        # æ·»åŠ å°å»¶è¿Ÿï¼Œé¿å…APIè¯·æ±‚è¿‡äºå¯†é›†
        time.sleep(0.05)
    
    return results


def main(start, end):
    try:
        print("=" * 60)
        print("ğŸš€ å¼€å§‹æ–‡æ¡£å¹¶å‘å¤„ç†æµç¨‹")
        print("=" * 60)

        # 1. åˆå§‹åŒ–èµ„æº
        print("\n1. åˆå§‹åŒ–ä¾èµ–èµ„æº")
        persona_retriever = PersonaRetriever(PERSONA_INDEX_DIR)
        
        # åˆå§‹åŒ–çº¿ç¨‹å®‰å…¨çš„å†™å…¥å™¨
        writer = ThreadSafeWriter(OUTPUT_JSONL_PATH)
        print("âœ… æ‰€æœ‰ä¾èµ–èµ„æºåˆå§‹åŒ–å®Œæˆ")

        # 2. è¯»å–è¾“å…¥æ–‡æ¡£
        print("\n2. è¯»å–è¾“å…¥æ–‡æ¡£")
        input_docs = load_jsonl(INPUT_JSONL_PATH, start, end)
        if not input_docs:
            print("âš ï¸ æ— æœ‰æ•ˆæ–‡æ¡£ï¼Œç¨‹åºé€€å‡º")
            return

        # 3. å¹¶å‘å¤„ç†é…ç½®
        max_workers = MAX_WORKERS  # å¹¶å‘çº¿ç¨‹æ•°
        batch_size = BATCH_SIZE    # æ¯æ‰¹å¤„ç†çš„æ–‡æ¡£æ•°
        
        print(f"\n3. å¼€å§‹å¹¶å‘å¤„ç†ï¼ˆå…±{len(input_docs)}ä¸ªæ–‡æ¡£ï¼‰")
        print(f"ğŸ“Š å¹¶å‘é…ç½®: {max_workers}ä¸ªçº¿ç¨‹ï¼Œæ¯æ‰¹{batch_size}ä¸ªæ–‡æ¡£")

        # 4. åˆ†æ‰¹å¤„ç†
        success_count = 0
        failed_count = 0
        
        # å°†æ–‡æ¡£åˆ†æ‰¹
        batches = []
        for i in range(0, len(input_docs), batch_size):
            batch = input_docs[i:i + batch_size]
            batches.append((batch, persona_retriever, i // batch_size))

        # ä½¿ç”¨çº¿ç¨‹æ± å¹¶å‘å¤„ç†
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            # æäº¤æ‰€æœ‰æ‰¹æ¬¡ä»»åŠ¡
            future_to_batch = {
                executor.submit(process_batch_concurrent, batch_args): batch_idx 
                for batch_idx, batch_args in enumerate(batches)
            }
            
            # å¤„ç†å®Œæˆçš„ä»»åŠ¡
            with tqdm(total=len(input_docs), desc="ğŸ“Š å¹¶å‘å¤„ç†è¿›åº¦") as pbar:
                for future in as_completed(future_to_batch):
                    batch_idx = future_to_batch[future]
                    try:
                        batch_results = future.result()
                        
                        # å¤„ç†æ‰¹æ¬¡ç»“æœ
                        for result in batch_results:
                            if result["status"] == "success":
                                writer.write_doc(result["data"])
                                success_count += 1
                            else:
                                failed_count += 1
                            pbar.update(1)
                            
                    except Exception as e:
                        print(f"âŒ æ‰¹æ¬¡ {batch_idx} å¤„ç†å¼‚å¸¸: {str(e)}")
                        # æ‰¹æ¬¡å¤±è´¥ï¼Œæ‰€æœ‰æ–‡æ¡£éƒ½æ ‡è®°ä¸ºå¤±è´¥
                        batch_size_actual = len(batches[batch_idx][0])
                        failed_count += batch_size_actual
                        pbar.update(batch_size_actual)

        print("\n4. å¤„ç†ç»Ÿè®¡æŠ¥å‘Š")
        total_count = len(input_docs)
        print(f"ğŸ“‹ ç»Ÿè®¡ç»“æœï¼š")
        print(f"   - æ€»å¤„ç†æ–‡æ¡£æ•°ï¼š{total_count}")
        print(f"   - æˆåŠŸæ•°ï¼š{success_count}ï¼ˆ{round(success_count / total_count * 100, 1)}%ï¼‰")
        print(f"   - å¤±è´¥æ•°ï¼š{failed_count}ï¼ˆ{round(failed_count / total_count * 100, 1)}%ï¼‰")
        print(f"   - è¾“å‡ºæ–‡ä»¶ï¼š{OUTPUT_JSONL_PATH}")

        print("\n" + "=" * 60)
        print("ğŸ‰ å¹¶å‘å¤„ç†æµç¨‹å®Œæˆ")
        print("=" * 60)

    except Exception as e:
        print(f"\nâŒ ç¨‹åºå…¨å±€å¼‚å¸¸ï¼š{str(e)}")
        traceback.print_exc()
        print("=" * 60)


if __name__ == "__main__":
    # é…ç½®å‚æ•°
    INPUT_JSONL_PATH = "./datasets/OmniEval-Corpus/all_data_clean_new.jsonl"
    OUTPUT_JSONL_PATH = "./datasets/embedding/querys.jsonl"
    PERSONA_INDEX_DIR = "./datasets/persona-hub/finance_persona_index"
    LLM_MODEL = "qwen3-30b-a3b-instruct-2507"
    SYSTEM_PROMPT = "ä½ æ˜¯é‡‘èé¢†åŸŸçš„ä¸“ä¸šåˆ†æåŠ©æ‰‹"
    
    # å¹¶å‘é…ç½® - æ ¹æ®ä½ çš„APIé™åˆ¶å’ŒæœåŠ¡å™¨æ€§èƒ½è°ƒæ•´
    MAX_WORKERS = 8      # å¹¶å‘çº¿ç¨‹æ•°ï¼Œå»ºè®®ä»4-8å¼€å§‹æµ‹è¯•
    BATCH_SIZE = 10      # æ¯æ‰¹å¤„ç†çš„æ–‡æ¡£æ•°ï¼Œå»ºè®®10-50

    import argparse

    parser = argparse.ArgumentParser(description="Embeddingè’¸é¦ç”Ÿæˆå·¥å…·")
    parser.add_argument("--start", type=int, default=0,
                        help="è’¸é¦èµ·å§‹è¡Œå·(0-based, inclusive)")
    parser.add_argument("--end", type=int, default=None,
                        help="è’¸é¦æˆªæ­¢è¡Œå·(0-based, exclusive)")
    args = parser.parse_args()

    main(args.start, args.end)
    # file = load_jsonl("../../datasets/OmniEval-Corpus/all_data_clean.jsonl")
    # print(file)
